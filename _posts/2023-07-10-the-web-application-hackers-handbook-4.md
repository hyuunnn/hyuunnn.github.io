---
layout: post
title: "웹 해킹 & 보안 완벽 가이드 - Chapter 04"
description: ""
date: 2023-07-10
tags: [book]
---

<a href="https://www.yes24.com/Product/Goods/14275829">웹 해킹 & 보안 완벽 가이드</a>

웹 사이트를 공격하기 전에 어떤 기능, 기술을 사용하는지 등을 조사해야 한다.

하지만 웹 사이트가 거대해지고 방대한 기능을 제공하면서 효율적인 지도 작성의 중요성이 커졌다.

이때 수작업으로 브라우징을 할 수도 있지만 자동화된 도구들이 많이 나와있다.

책의 저자가 Burp Suite 개발자인만큼 해당 도구에 대한 설명이 나온다.

## Web Spidering

웹 페이지에 다른 컨텐츠의 링크가 있는지 파싱하고, 다시 요청하여 더 이상 새로운 컨텐츠가 없을 때까지 반복한다.

HTML Form에 임의의 값을 넣어서 요청을 보내기도 한다고 한다. (Form에 의해 접속하는 URL 추출)

하지만 자동화의 한계도 존재한다.

정교한 입력을 요구하는 Form이나 자바스크립트를 통해 동적으로 생성되는 경우 제대로 처리하지 못한다.

또한 action에 따라서 다르게 동작하는 POST 페이지가 있다고 가정했을 때, 스파이더링은 한 번 요청됐던 URL은 다시 요청하지 않게 설계되어 있기 때문에 중요한 정보를 놓칠 수도 있다.

스파이더 도구가 로그아웃 페이지에 접속하여 세션이 중단될 수도 있고, 접속한 페이지가 서버를 재시작하거나 사용자를 삭제하는 등의 동작을 수행한다면 큰 문제가 발생할 수 있다.

URL에 존재하는 매개변수 값에 따라서 일회성 정보(타이머, 난수 등)를 반환하는 페이지의 경우에는 각 URL이 다른 URL이라고 판단하여 무한으로 스파이더링을 수행할 수도 있다.

## User-Directed Spidering

Burp Suite에서 사용하는 방법이다. 사용자가 일반적으로 브라우징하듯이 여러 링크들을 접속하면서 주고받은 내용들은 프록시를 거쳐서 Burp Suite에 저장된다.

그렇게 방문한 URL들을 스파이더링하여 새로운 컨텐츠들을 찾는다.

사용자가 Form에서 요구하는 조건에 맞게 입력하여 동적인 페이지들도 찾을 수 있다.

권한을 요구하는 페이지의 경우에도 사용자가 로그인을 하면 세션을 유지하면서 스파이더링을 할 수 있다. (selenium을 사용하는 이유이기도 하다.)

**완전 자동화는 쉽지 않고, 최소한의 수작업이 필요하다.** -> 수작업으로 접속한 URL을 바탕으로 스파이더 도구가 수집하며, 수집한 URL을 다시 수동으로 열어보면서 새로운 컨텐츠를 찾는다. (이때 위에서 설명했던 문제가 발생할 수 있는 URL은 접근하지 않게 설정해야 한다.)

## Burp Intruder
